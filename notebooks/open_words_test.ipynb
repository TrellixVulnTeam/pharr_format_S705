{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'open_words.parse'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-0af634489a7f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcollections\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCounter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimporter\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCorpusImporter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mopen_words\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mParse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformatter\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0massemble_phi5_author_filepaths\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformatter\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mphi5_plaintext_cleanup\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'open_words.parse'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from tqdm import tqdm \n",
    "from docx import Document\n",
    "from docx.oxml import OxmlElement\n",
    "from docx.oxml.ns import qn\n",
    "from collections import Counter\n",
    "from cltk.corpus.utils.importer import CorpusImporter\n",
    "from open_words.parse import Parse\n",
    "from cltk.corpus.utils.formatter import assemble_phi5_author_filepaths\n",
    "from cltk.corpus.utils.formatter import phi5_plaintext_cleanup\n",
    "from cltk.stem.latin.j_v import JVReplacer\n",
    "from collections import Counter\n",
    "from nltk.tokenize.punkt import PunktLanguageVars\n",
    "from cltk.lemmatize.latin.backoff import BackoffLatinLemmatizer\n",
    "from cltk.corpus.readers import get_corpus_reader\n",
    "import sys \n",
    "\n",
    "sys.path.append('/Users/aleedom/cltk/open_words/')\n",
    "\n",
    "lemmatizer = BackoffLatinLemmatizer()\n",
    "\n",
    "path = 'lexica/Lewis_Short_XML/lat.ls.perseus-eng1.xml'\n",
    "\n",
    "corpus_importer = CorpusImporter('latin')\n",
    "corpus_importer.list_corpora\n",
    "corpus_importer.import_corpus('latin_text_perseus')\n",
    "\n",
    "reader = get_corpus_reader(language='latin', corpus_name='latin_text_latin_library')\n",
    "reader._fileids = ['ammianus/14.txt'] # ammianus book 14\n",
    "\n",
    "stops = list(stops[stops['cumsum'] < .705].lemma) # set stop limit \n",
    "\n",
    "paras = list(reader.paras())\n",
    "paras = [item for sublist in paras for item in sublist]\n",
    "\n",
    "numbers = ('1', '2', '3', '4', '5', '6', '7', '8', '9')\n",
    "punc = ['.', ',', ';', '\"', \"'\", '-que', '-ne', '-ve']\n",
    "\n",
    "doc = Document()\n",
    "parser = Parse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####\n",
    "## REFINEMENTS\n",
    "#### \n",
    "\n",
    "# gender appearing on verbs \n",
    "# bold type for vocab words \n",
    "# alphabetize vocab words \n",
    "\n",
    "\n",
    "def divide_chunks(l, n): \n",
    "      \n",
    "    # looping till length l \n",
    "    for i in range(0, len(l), n-1):  \n",
    "        yield l[i:i + n]\n",
    "\n",
    "def parse_paragraph(paragraph: 'str') -> \"str\":\n",
    "    \"\"\"Function to take paragraph of a Latin text and return a dictionary including definitions (but not citations). \n",
    "    The goal is to use this function to create short entries for a paragraph of a text. We can then use the paragraphs to build our Pharr formatted document. \n",
    "    \n",
    "    :param paragraph: paragraph of parsed text\n",
    "    \"\"\"\n",
    "    \n",
    "    in_list = paragraph.split(' ')\n",
    "    # lemmatize \n",
    "    in_list = [lemmatizer.lemmatize([x])[0][1] for x in in_list]\n",
    "    in_list = [_ for _ in in_list if _ not in stops]\n",
    "    out_str = '' \n",
    "    path = 'lexica/Lewis_Short_XML/lat.ls.perseus-eng1.xml'\n",
    "    tree = ET.parse(path)\n",
    "    entries = tree.xpath('//entryFree')\n",
    "    endings = ''\n",
    "    gender = \"\"\n",
    "    out_list = []\n",
    "    \n",
    "    for word in in_list: \n",
    "        lemma = lemmatizer.lemmatize([word])[0][1]\n",
    "        \n",
    "        for entry in entries:\n",
    "            senses = []\n",
    "            if entry.get('key') == lemma:\n",
    "                if entry.find('itype') is not None: \n",
    "                    endings = f'{lemma} {entry.find(\"itype\").text}'\n",
    "                if entry.find('gen') is not None: \n",
    "                    gender = entry.find('gen').text\n",
    "                for sense in entry.findall('sense')[:4]:\n",
    "                    # print(sense.get('level'))\n",
    "                    if sense.get(\"level\") in ['1', '2']:\n",
    "                        for tr in sense.findall('hi')[1:3]:\n",
    "                            senses.append(tr.text)\n",
    "#                 print(senses)\n",
    "                if endings != '':\n",
    "                    out_string = f\"\"\"{endings} {gender}: {'; '.join(senses).strip('., ')}\"\"\"\n",
    "                else:\n",
    "                    out_string = f'{lemma} {gender}: {\"; \".join(senses).strip(\"., \")}'\n",
    "                if senses == []:\n",
    "                    pass \n",
    "                else:\n",
    "                    out_list.append(out_string)\n",
    "    return '\\n'.join(out_list)\n",
    "\n",
    "def clean_paragraph(ls):\n",
    "    out = f\"\"\n",
    "    for i in range(len(ls) - 1):  \n",
    "        if ls[i + 1] in punc: \n",
    "            out += ''.join([ls[i], ls[i + 1].strip('-')]) + ' '\n",
    "        elif ls[i] not in punc:\n",
    "            out += f'{ls[i]} '\n",
    "            \n",
    "        else: \n",
    "            pass\n",
    "    return out\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = divide_chunks(paras, 150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "br = '———————————————————————————————————————'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [05:46,  5.42s/it]\n"
     ]
    }
   ],
   "source": [
    "from docx.shared import Inches, Cm\n",
    "doc = Document()\n",
    "for i in tqdm(x): \n",
    "    p = clean_paragraph(i)\n",
    "    t = parse_paragraph(p)\n",
    "    doc.add_paragraph(p)\n",
    "    doc.add_paragraph(br)\n",
    "    doc.add_section(0)\n",
    "    \n",
    "    section = doc.sections[-1]\n",
    "\n",
    "    sectPr = section._sectPr\n",
    "    cols = sectPr.xpath('./w:cols')[0]\n",
    "    cols.set(qn('w:num'),'2')\n",
    "    doc.add_paragraph(t)\n",
    "\n",
    "    doc.add_section(0)\n",
    "    section = doc.sections[-1]\n",
    "\n",
    "    sectPr = section._sectPr\n",
    "    cols = sectPr.xpath('./w:cols')[0]\n",
    "    cols.set(qn('w:num'),'1')\n",
    "    doc.add_page_break()\n",
    "    \n",
    "# clear margins \n",
    "\n",
    "sections = doc.sections\n",
    "for section in sections:\n",
    "    section.top_margin = Inches(0.5)\n",
    "    section.bottom_margin = Inches(0.5)\n",
    "    section.left_margin = Inches(.75)\n",
    "    section.right_margin = Inches(.75)\n",
    "\n",
    "doc.save('test.docx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = '''de Caesare quisque sentiret. et haec confidenter agebat in urbe ubi pernoctantium luminum claritudo dierum solet imitari fulgorem. postremo agnitus saepe iamque -que, si prodisset, conspicuum se fore contemplans, non nisi luce palam egrediens ad agenda quae putabat seria cernebatur. et haec quidem medullitus multis gementibus agebantur. 10. Thalassius vero ea tempestate praefectus praetorio praesens ipse quoque adrogantis ingenii, considerans incitationem eius ad multorum augeri discrimina, non maturitate vel consiliis mitigabat, ut aliquotiens celsae potestates iras principum molliverunt, sed adversando iurgandoque cum parum congrueret, eum ad rabiem potius evibrabat, Augustum actus eius exaggerando creberrime docens, idque -que, incertum qua mente, ne lateret adfectans. quibus mox Caesar acrius efferatus, velut contumaciae quoddam vexillum altius erigens, sine respectu salutis alienae vel suae ad vertenda opposita instar rapidi fluminis irrevocabili impetu ferebatur. '\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
