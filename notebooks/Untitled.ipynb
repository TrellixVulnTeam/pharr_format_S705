{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm \n",
    "from docx import Document\n",
    "\n",
    "from docx.oxml import OxmlElement\n",
    "from docx.oxml.ns import qn\n",
    "\n",
    "doc = Document()\n",
    "\n",
    "from cltk.corpus.utils.formatter import assemble_phi5_author_filepaths\n",
    "from cltk.corpus.utils.formatter import phi5_plaintext_cleanup\n",
    "from cltk.stem.latin.j_v import JVReplacer\n",
    "from collections import Counter\n",
    "from nltk.tokenize.punkt import PunktLanguageVars\n",
    "import os\n",
    "from cltk.lemmatize.latin.backoff import BackoffLatinLemmatizer\n",
    "lemmatizer = BackoffLatinLemmatizer()\n",
    "\n",
    "path = 'lexica/Lewis_Short_XML/lat.ls.perseus-eng1.xml'\n",
    "path2 = 'lexica/Lewis_Short_XML/lat.ls.perseus-eng2.xml'\n",
    "\n",
    "from collections import Counter\n",
    "from cltk.corpus.utils.importer import CorpusImporter\n",
    "corpus_importer = CorpusImporter('latin')\n",
    "corpus_importer.list_corpora\n",
    "\n",
    "from cltk.corpus.readers import get_corpus_reader\n",
    "corpus_importer.import_corpus('latin_text_perseus')\n",
    "reader = get_corpus_reader(language='latin', corpus_name='latin_text_latin_library')\n",
    "\n",
    "stops = list(stops[stops['cumsum'] < .705].lemma) # set stop limit \n",
    "\n",
    "reader._fileids = ['ammianus/14.txt'] # ammianus book 14\n",
    "\n",
    "paras = list(reader.paras())\n",
    "\n",
    "paras = [item for sublist in paras for item in sublist]\n",
    "\n",
    "numbers = ('1', '2', '3', '4', '5', '6', '7', '8', '9')\n",
    "punc = ['.', ',', ';', '\"', \"'\", '-que', '-ne', '-ve']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/latin_word_counts.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lemma</th>\n",
       "      <th>token_count</th>\n",
       "      <th>pct_of_tokens</th>\n",
       "      <th>cumsum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sum</td>\n",
       "      <td>132427</td>\n",
       "      <td>0.030682</td>\n",
       "      <td>0.030682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>et</td>\n",
       "      <td>121212</td>\n",
       "      <td>0.028084</td>\n",
       "      <td>0.058766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>qui</td>\n",
       "      <td>109323</td>\n",
       "      <td>0.025329</td>\n",
       "      <td>0.084096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>in</td>\n",
       "      <td>78831</td>\n",
       "      <td>0.018265</td>\n",
       "      <td>0.102360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-que</td>\n",
       "      <td>65771</td>\n",
       "      <td>0.015239</td>\n",
       "      <td>0.117599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>is</td>\n",
       "      <td>55632</td>\n",
       "      <td>0.012890</td>\n",
       "      <td>0.130489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>non</td>\n",
       "      <td>48894</td>\n",
       "      <td>0.011328</td>\n",
       "      <td>0.141817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>hic</td>\n",
       "      <td>48074</td>\n",
       "      <td>0.011138</td>\n",
       "      <td>0.152955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ut</td>\n",
       "      <td>37646</td>\n",
       "      <td>0.008722</td>\n",
       "      <td>0.161678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>cum2</td>\n",
       "      <td>35809</td>\n",
       "      <td>0.008297</td>\n",
       "      <td>0.169974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>ego</td>\n",
       "      <td>32900</td>\n",
       "      <td>0.007623</td>\n",
       "      <td>0.177597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>tu</td>\n",
       "      <td>32881</td>\n",
       "      <td>0.007618</td>\n",
       "      <td>0.185215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>ad</td>\n",
       "      <td>29883</td>\n",
       "      <td>0.006924</td>\n",
       "      <td>0.192139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>-ne</td>\n",
       "      <td>28493</td>\n",
       "      <td>0.006602</td>\n",
       "      <td>0.198741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>ille</td>\n",
       "      <td>27959</td>\n",
       "      <td>0.006478</td>\n",
       "      <td>0.205219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>ab</td>\n",
       "      <td>27819</td>\n",
       "      <td>0.006445</td>\n",
       "      <td>0.211664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>si</td>\n",
       "      <td>26634</td>\n",
       "      <td>0.006171</td>\n",
       "      <td>0.217835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>neque</td>\n",
       "      <td>24929</td>\n",
       "      <td>0.005776</td>\n",
       "      <td>0.223611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>sed</td>\n",
       "      <td>23968</td>\n",
       "      <td>0.005553</td>\n",
       "      <td>0.229164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>atque</td>\n",
       "      <td>23226</td>\n",
       "      <td>0.005381</td>\n",
       "      <td>0.234545</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    lemma  token_count  pct_of_tokens    cumsum\n",
       "0     sum       132427       0.030682  0.030682\n",
       "1      et       121212       0.028084  0.058766\n",
       "2     qui       109323       0.025329  0.084096\n",
       "3      in        78831       0.018265  0.102360\n",
       "4    -que        65771       0.015239  0.117599\n",
       "5      is        55632       0.012890  0.130489\n",
       "6     non        48894       0.011328  0.141817\n",
       "7     hic        48074       0.011138  0.152955\n",
       "8      ut        37646       0.008722  0.161678\n",
       "9    cum2        35809       0.008297  0.169974\n",
       "10    ego        32900       0.007623  0.177597\n",
       "11     tu        32881       0.007618  0.185215\n",
       "12     ad        29883       0.006924  0.192139\n",
       "13    -ne        28493       0.006602  0.198741\n",
       "14   ille        27959       0.006478  0.205219\n",
       "15     ab        27819       0.006445  0.211664\n",
       "16     si        26634       0.006171  0.217835\n",
       "17  neque        24929       0.005776  0.223611\n",
       "18    sed        23968       0.005553  0.229164\n",
       "19  atque        23226       0.005381  0.234545"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'this is:test text'"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "document.save('bold_test.docx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('subditicii', 'subditicii')]"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize(['subditicii'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####\n",
    "## REFINEMENTS\n",
    "#### \n",
    "\n",
    "# gender appearing on verbs \n",
    "# bold type for vocab words \n",
    "# alphabetize vocab words \n",
    "\n",
    "\n",
    "def divide_chunks(l, n): \n",
    "      \n",
    "    # looping till length l \n",
    "    for i in range(0, len(l), n-1):  \n",
    "        yield l[i:i + n]\n",
    "\n",
    "def parse_paragraph(paragraph: 'str') -> \"str\":\n",
    "    \"\"\"Function to take paragraph of a Latin text and return a dictionary including definitions (but not citations). \n",
    "    The goal is to use this function to create short entries for a paragraph of a text. We can then use the paragraphs to build our Pharr formatted document. \n",
    "    \n",
    "    :param paragraph: paragraph of parsed text\n",
    "    \"\"\"\n",
    "    \n",
    "    in_list = paragraph.split(' ')\n",
    "    # lemmatize \n",
    "    in_list = [lemmatizer.lemmatize([x])[0][1] for x in in_list]\n",
    "    in_list = [_ for _ in in_list if _ not in stops]\n",
    "    out_str = '' \n",
    "    path = 'lexica/Lewis_Short_XML/lat.ls.perseus-eng1.xml'\n",
    "    tree = ET.parse(path)\n",
    "    entries = tree.xpath('//entryFree')\n",
    "    endings = ''\n",
    "    gender = \"\"\n",
    "    out_list = []\n",
    "    \n",
    "    for word in in_list: \n",
    "        lemma = lemmatizer.lemmatize([word])[0][1]\n",
    "        \n",
    "        for entry in entries:\n",
    "            senses = []\n",
    "            if entry.get('key') == lemma:\n",
    "                if entry.find('itype') is not None: \n",
    "                    endings = f'{lemma} {entry.find(\"itype\").text}'\n",
    "                if entry.find('gen') is not None: \n",
    "                    gender = entry.find('gen').text\n",
    "                for sense in entry.findall('sense')[:4]:\n",
    "                    # print(sense.get('level'))\n",
    "                    if sense.get(\"level\") in ['1', '2']:\n",
    "                        for tr in sense.findall('hi')[1:3]:\n",
    "                            senses.append(tr.text)\n",
    "#                 print(senses)\n",
    "                if endings != '':\n",
    "                    out_string = f\"\"\"{endings} {gender}: {'; '.join(senses).strip('., ')}\"\"\"\n",
    "                else:\n",
    "                    out_string = f'{lemma} {gender}: {\"; \".join(senses).strip(\"., \")}'\n",
    "                if senses == []:\n",
    "                    pass \n",
    "                else:\n",
    "                    out_list.append(out_string)\n",
    "    return '\\n'.join(out_list)\n",
    "\n",
    "def clean_paragraph(ls):\n",
    "    out = f\"\"\n",
    "    for i in range(len(ls) - 1):  \n",
    "        if ls[i + 1] in punc: \n",
    "            out += ''.join([ls[i], ls[i + 1].strip('-')]) + ' '\n",
    "        elif ls[i] not in punc:\n",
    "            out += f'{ls[i]} '\n",
    "            \n",
    "        else: \n",
    "            pass\n",
    "    return out\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = divide_chunks(paras, 150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "br = '———————————————————————————————————————'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [05:46,  5.42s/it]\n"
     ]
    }
   ],
   "source": [
    "from docx.shared import Inches, Cm\n",
    "doc = Document()\n",
    "for i in tqdm(x): \n",
    "    p = clean_paragraph(i)\n",
    "    t = parse_paragraph(p)\n",
    "    doc.add_paragraph(p)\n",
    "    doc.add_paragraph(br)\n",
    "    doc.add_section(0)\n",
    "    \n",
    "    section = doc.sections[-1]\n",
    "\n",
    "    sectPr = section._sectPr\n",
    "    cols = sectPr.xpath('./w:cols')[0]\n",
    "    cols.set(qn('w:num'),'2')\n",
    "    doc.add_paragraph(t)\n",
    "\n",
    "    doc.add_section(0)\n",
    "    section = doc.sections[-1]\n",
    "\n",
    "    sectPr = section._sectPr\n",
    "    cols = sectPr.xpath('./w:cols')[0]\n",
    "    cols.set(qn('w:num'),'1')\n",
    "    doc.add_page_break()\n",
    "    \n",
    "# clear margins \n",
    "\n",
    "sections = doc.sections\n",
    "for section in sections:\n",
    "    section.top_margin = Inches(0.5)\n",
    "    section.bottom_margin = Inches(0.5)\n",
    "    section.left_margin = Inches(.75)\n",
    "    section.right_margin = Inches(.75)\n",
    "\n",
    "doc.save('test.docx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = '''de Caesare quisque sentiret. et haec confidenter agebat in urbe ubi pernoctantium luminum claritudo dierum solet imitari fulgorem. postremo agnitus saepe iamque -que, si prodisset, conspicuum se fore contemplans, non nisi luce palam egrediens ad agenda quae putabat seria cernebatur. et haec quidem medullitus multis gementibus agebantur. 10. Thalassius vero ea tempestate praefectus praetorio praesens ipse quoque adrogantis ingenii, considerans incitationem eius ad multorum augeri discrimina, non maturitate vel consiliis mitigabat, ut aliquotiens celsae potestates iras principum molliverunt, sed adversando iurgandoque cum parum congrueret, eum ad rabiem potius evibrabat, Augustum actus eius exaggerando creberrime docens, idque -que, incertum qua mente, ne lateret adfectans. quibus mox Caesar acrius efferatus, velut contumaciae quoddam vexillum altius erigens, sine respectu salutis alienae vel suae ad vertenda opposita instar rapidi fluminis irrevocabili impetu ferebatur. '\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "125720"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
