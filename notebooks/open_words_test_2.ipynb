{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modules \n",
    "import pandas as pd\n",
    "import os\n",
    "import sys \n",
    "\n",
    "sys.path.append('/Users/aleedom/cltk/open_words/')\n",
    "from open_words.parse import Parse\n",
    "\n",
    "from tqdm import tqdm \n",
    "\n",
    "from docx import Document\n",
    "from docx.oxml import OxmlElement\n",
    "from docx.oxml.ns import qn\n",
    "from docx.shared import Pt\n",
    "\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "from cltk.corpus.utils.importer import CorpusImporter\n",
    "\n",
    "from cltk.corpus.utils.formatter import assemble_phi5_author_filepaths\n",
    "from cltk.corpus.utils.formatter import phi5_plaintext_cleanup\n",
    "from cltk.stem.latin.j_v import JVReplacer\n",
    "from nltk.tokenize.punkt import PunktLanguageVars\n",
    "from cltk.lemmatize.latin.backoff import BackoffLatinLemmatizer\n",
    "from cltk.corpus.readers import get_corpus_reader\n",
    "\n",
    "# variables\n",
    "\n",
    "lemmatizer = BackoffLatinLemmatizer()\n",
    "\n",
    "path = '../lexica/Lewis_Short_XML/lat.ls.perseus-eng1.xml'\n",
    "\n",
    "corpus_importer = CorpusImporter('latin')\n",
    "corpus_importer.list_corpora\n",
    "corpus_importer.import_corpus('latin_text_perseus')\n",
    "\n",
    "reader = get_corpus_reader(language='latin', corpus_name='latin_text_latin_library')\n",
    "reader._fileids = ['ammianus/14.txt'] # ammianus book 14\n",
    "\n",
    "stops = pd.read_csv('../data/latin_word_counts.csv')\n",
    "stops = list(stops[stops['cumsum'] < .705].lemma) # set stop limit \n",
    "\n",
    "paras = list(reader.paras())\n",
    "paras = [item for sublist in paras for item in sublist]\n",
    "\n",
    "numbers = ('1', '2', '3', '4', '5', '6', '7', '8', '9')\n",
    "punc = ['.', ',', ';', '\"', \"'\", '-que', '-ne', '-ve']\n",
    "\n",
    "doc = Document()\n",
    "parser = Parse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function divide_chunks in module __main__:\n",
      "\n",
      "divide_chunks(l, n)\n",
      "\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####\n",
    "## REFINEMENTS\n",
    "#### \n",
    "\n",
    "# gender appearing on verbs \n",
    "# bold type for vocab words \n",
    "# alphabetize vocab words \n",
    "\n",
    "\n",
    "def divide_chunks(l, n): \n",
    "      \n",
    "    # looping till length l \n",
    "    for i in range(0, len(l), n-1):  \n",
    "        yield l[i:i + n]\n",
    "\n",
    "def parse_paragraph(paragraph: 'str') -> \"str\":\n",
    "    \"\"\"Function to take paragraph of a Latin text and return a dictionary including definitions (but not citations). \n",
    "    The goal is to use this function to create short entries for a paragraph of a text. We can then use the paragraphs to build our Pharr formatted document. \n",
    "    \n",
    "    :param paragraph: paragraph of parsed text\n",
    "    \"\"\"\n",
    "    \n",
    "    in_list = paragraph.split(' ')\n",
    "    # lemmatize \n",
    "    in_list = [lemmatizer.lemmatize([x])[0][1] for x in in_list]\n",
    "    in_list = [_ for _ in in_list if _ not in stops]\n",
    "    out_str = '' \n",
    "    path = '../lexica/Lewis_Short_XML/lat.ls.perseus-eng1.xml'\n",
    "    tree = ET.parse(path)\n",
    "    entries = tree.xpath('//entryFree')\n",
    "    endings = ''\n",
    "    gender = \"\"\n",
    "    out_list = []\n",
    "    \n",
    "    for word in in_list: \n",
    "        lemma = lemmatizer.lemmatize([word])[0][1]\n",
    "        \n",
    "        for entry in entries:\n",
    "            senses = []\n",
    "            if entry.get('key') == lemma:\n",
    "                if entry.find('itype') is not None: \n",
    "                    endings = f'{lemma} {entry.find(\"itype\").text}'\n",
    "                if entry.find('gen') is not None: \n",
    "                    gender = entry.find('gen').text\n",
    "                for sense in entry.findall('sense')[:4]:\n",
    "                    # print(sense.get('level'))\n",
    "                    if sense.get(\"level\") in ['1', '2']:\n",
    "                        for tr in sense.findall('hi')[1:3]:\n",
    "                            senses.append(tr.text)\n",
    "#                 print(senses)\n",
    "                if endings != '':\n",
    "                    out_string = f\"\"\"{endings} {gender}: {'; '.join(senses).strip('., ')}\"\"\"\n",
    "                else:\n",
    "                    out_string = f'{lemma} {gender}: {\"; \".join(senses).strip(\"., \")}'\n",
    "                if senses == []:\n",
    "                    pass \n",
    "                else:\n",
    "                    out_list.append(out_string)\n",
    "    return '\\n'.join(out_list)\n",
    "\n",
    "def clean_paragraph(ls):\n",
    "    out = f\"\"\n",
    "    for i in range(len(ls) - 1):  \n",
    "        if ls[i + 1] in punc: \n",
    "            out += ''.join([ls[i], ls[i + 1].strip('-')]) + ' '\n",
    "        elif ls[i] not in punc:\n",
    "            out += f'{ls[i]} '\n",
    "            \n",
    "        else: \n",
    "            pass\n",
    "    return out.strip(str(punc)) # added .strip(punc) to clear '-que' '-ne' etc.\n",
    "\n",
    "# helper function from stackoverflow https://stackoverflow.com/questions/6039103/counting-depth-or-the-deepest-level-a-nested-list-goes-to\n",
    "\n",
    "def depth(l):\n",
    "    if isinstance(l, list):\n",
    "        return 1 + max(depth(item) for item in l)\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def flatten_paragraphs(paras): \n",
    "    while depth(paras) >= 2: \n",
    "        paras = [item for sublist in paras for item in sublist]\n",
    "    return paras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "paras = list(reader.paras())\n",
    "\n",
    "paras = flatten_paragraphs(paras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'word': 'insuperabilis',\n",
       " 'dictionary_form': ['insuperabile', 'insuperabil'],\n",
       " 'senses': ['insurmountable', 'unconquerable']}"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  out_dict['insuperabilis'] # test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## todo: \n",
    "- fix endings so they're included \n",
    "- ensure stop words are registered properly\n",
    "\n",
    "Otherwise, works well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'w': {'pos': 'N',\n",
       "   'n': [3, 1],\n",
       "   'parts': ['pavor', 'pavor'],\n",
       "   'senses': ['fear, panic'],\n",
       "   'form': '3 1 M T',\n",
       "   'orth': 'pavor',\n",
       "   'id': 29589},\n",
       "  'stems': [{'st': {'pos': 'N',\n",
       "     'form': '3 1 M T',\n",
       "     'orth': 'pavor',\n",
       "     'n': [3, 1],\n",
       "     'wid': 29589},\n",
       "    'infls': [{'ending': 'e',\n",
       "      'pos': 'N',\n",
       "      'note': '',\n",
       "      'n': [3, 0],\n",
       "      'form': 'LOC S X'},\n",
       "     {'ending': 'e', 'pos': 'N', 'note': '', 'n': [3, 0], 'form': 'DAT S X'},\n",
       "     {'ending': 'e', 'pos': 'N', 'note': '', 'n': [3, 0], 'form': 'ABL S C'},\n",
       "     {'ending': 'e',\n",
       "      'pos': 'N',\n",
       "      'note': 'stem_ends_in_cons',\n",
       "      'n': [3, 2],\n",
       "      'form': 'ABL S N'},\n",
       "     {'ending': 'e',\n",
       "      'pos': 'N',\n",
       "      'note': 'greek',\n",
       "      'n': [3, 8],\n",
       "      'form': 'VOC S X'}]}]}]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser.latin_to_english('pavore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "77it [1:23:46, 65.28s/it] \n"
     ]
    }
   ],
   "source": [
    "from docx.shared import Inches, Cm\n",
    "\n",
    "path = '../lexica/Lewis_Short_XML/lat.ls.perseus-eng1.xml'\n",
    "tree = ET.parse(path)\n",
    "entries = tree.xpath('//entryFree')\n",
    "\n",
    "\n",
    "x = divide_chunks(paras, 125)\n",
    "\n",
    "doc = Document()\n",
    "br = '———————————————————————————————————————'\n",
    "for i in tqdm(x): \n",
    "    passage_dict = {}\n",
    "    # format text \n",
    "    p = clean_paragraph(i)\n",
    "    \n",
    "    # add text to document \n",
    "    main_paragraph = doc.add_paragraph().add_run(p)\n",
    "    main_paragraph.font.size = Pt(12)\n",
    "    # visual line break \n",
    "    p2 = doc.add_paragraph()\n",
    "    p2.paragraph_format.alignment = WD_ALIGN_PARAGRAPH.CENTER\n",
    "    p2.add_run(br)\n",
    "    \n",
    "    # change formatting\n",
    "    \n",
    "    doc.add_section(0)\n",
    "    \n",
    "    section = doc.sections[-1]\n",
    "\n",
    "    sectPr = section._sectPr\n",
    "    cols = sectPr.xpath('./w:cols')[0]\n",
    "    cols.set(qn('w:num'),'2')\n",
    "    \n",
    "    # dictionary lookup \n",
    "    for w in i: \n",
    "        endings = []\n",
    "        gender = []\n",
    "        \n",
    "        # create dictionary for entries on the page \n",
    "        \n",
    "        lemma = lemmatizer.lemmatize([w])[0][1]\n",
    "        if lemma in stops: # expensive but maybe saves lookup time in the next step\n",
    "            continue # should need to add all the remaining words, though there is doubtless some slippage between the two systems \n",
    "        else: \n",
    "            parsed = parser.parse(w)\n",
    "            word = parsed['word']\n",
    "            # word \n",
    "            passage_dict[word] = {}\n",
    "            passage_dict[word]['word'] = word\n",
    "            passage_dict[word]['lemma'] = lemma\n",
    "            if len(parsed['defs']) >=1:\n",
    "                passage_dict[word]['senses'] = parsed['defs'][0]['senses']\n",
    "            else:\n",
    "                continue\n",
    "            # inflection\n",
    "            for entry in entries:\n",
    "                if entry.get('key') == lemma:\n",
    "                    if entry.find('itype') is not None: \n",
    "                        endings.append(f'{entry.find(\"itype\").text} ')\n",
    "                    if entry.find('gen') is not None: \n",
    "                        endings.append(entry.find('gen').text)\n",
    "            passage_dict[word]['dictionary_form'] = endings\n",
    "    \n",
    "    # now format dictionary for page \n",
    "    \n",
    "    # cleanup so the dictionary won't break \n",
    "    delete = [k for k in passage_dict.keys() if passage_dict[k].get('dictionary_form') is None]\n",
    "    \n",
    "    for k in delete: \n",
    "        del passage_dict[k]\n",
    "    \n",
    "    # sort vocab words so they're easy to find on the page   \n",
    "    vocab_words_sorted = sorted(list(passage_dict.keys()))\n",
    "    \n",
    "    for word in vocab_words_sorted:\n",
    "        bold_text = f'{\" \".join(passage_dict[word][\"dictionary_form\"]).strip(\".\")}: '\n",
    "        plain_text = f\"{', '.join(passage_dict[word]['senses'])}\" \n",
    "        \n",
    "        p = doc.add_paragraph()\n",
    "        \n",
    "        \n",
    "        bold_run = p.add_run()\n",
    "        bold_run.bold = True \n",
    "        bold_run.text = f'{passage_dict[word][\"lemma\"]} ' + bold_text\n",
    "        bold_run.font.size = Pt(10)\n",
    "        \n",
    "        plain_run = p.add_run()\n",
    "        plain_run.bold = False\n",
    "        plain_run.text = plain_text\n",
    "        plain_run.font.size = Pt(10)\n",
    "        \n",
    "        p.paragraph_format.space_after = Pt(0)\n",
    "        \n",
    "    # reset column formatting and move to next page \n",
    "    \n",
    "    doc.add_section(0)\n",
    "    section = doc.sections[-1]\n",
    "\n",
    "    sectPr = section._sectPr\n",
    "    cols = sectPr.xpath('./w:cols')[0]\n",
    "    cols.set(qn('w:num'),'1')\n",
    "    doc.add_page_break()\n",
    "    \n",
    "sections = doc.sections\n",
    "for section in sections:\n",
    "    section.top_margin = Inches(0.5)\n",
    "    section.bottom_margin = Inches(0.5)\n",
    "    section.left_margin = Inches(.75)\n",
    "    section.right_margin = Inches(.75)\n",
    "    \n",
    "doc.save('test_2.docx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lookup_words_in_paragraph(paragraph: 'list') -> 'dictionary': \n",
    "    \"\"\"Uses open words to define words\"\"\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'word': 'campestrem',\n",
       " 'defs': [{'orth': ['campestris', 'campestr'],\n",
       "   'senses': ['deities who presided over contests/games (pl.)',\n",
       "    'country deities'],\n",
       "   'infls': [{'ending': 'em',\n",
       "     'pos': 'noun',\n",
       "     'form': {'declension': 'accusative',\n",
       "      'number': 'singular',\n",
       "      'gender': 'C'}},\n",
       "    {'ending': 'em',\n",
       "     'pos': 'noun',\n",
       "     'form': {'declension': 'accusative',\n",
       "      'number': 'singular',\n",
       "      'gender': ''}}]},\n",
       "  {'orth': ['campestre', 'campestr'],\n",
       "   'senses': ['flat/level country/ground (pl.)', 'plains'],\n",
       "   'infls': [{'ending': 'em',\n",
       "     'pos': 'noun',\n",
       "     'form': {'declension': 'accusative',\n",
       "      'number': 'singular',\n",
       "      'gender': 'C'}},\n",
       "    {'ending': 'em',\n",
       "     'pos': 'noun',\n",
       "     'form': {'declension': 'accusative',\n",
       "      'number': 'singular',\n",
       "      'gender': ''}}]},\n",
       "  {'orth': ['campestre', 'campestr'],\n",
       "   'senses': ['level, even, flat, of level field',\n",
       "    'on open plain/field',\n",
       "    'plain-dwelling'],\n",
       "   'infls': [{'ending': 'em',\n",
       "     'pos': 'adjective',\n",
       "     'form': {'declension': 'accusative',\n",
       "      'number': 'singular',\n",
       "      'gender': 'C'}}]},\n",
       "  {'orth': ['campester', 'campestr'],\n",
       "   'senses': ['level, even, flat, of level field',\n",
       "    'on open plain/field',\n",
       "    'plain-dwelling'],\n",
       "   'infls': [{'ending': 'em',\n",
       "     'pos': 'adjective',\n",
       "     'form': {'declension': 'accusative',\n",
       "      'number': 'singular',\n",
       "      'gender': 'C'}}]}]}"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser.parse('campestrem')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "br = '———————————————————————————————————————'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:57, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-1e9c8e90b445>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclean_paragraph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparse_paragraph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_paragraph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_paragraph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-30-5d2df3c57f4b>\u001b[0m in \u001b[0;36mparse_paragraph\u001b[0;34m(paragraph)\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mentry\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m             \u001b[0msenses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mentry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'key'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlemma\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mentry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'itype'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m                     \u001b[0mendings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf'{lemma} {entry.find(\"itype\").text}'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from docx.shared import Inches, Cm\n",
    "doc = Document()\n",
    "for i in tqdm(x): \n",
    "    p = clean_paragraph(i)\n",
    "    t = parse_paragraph(p)\n",
    "    doc.add_paragraph(p)\n",
    "    doc.add_paragraph(br)\n",
    "    doc.add_section(0)\n",
    "    \n",
    "    section = doc.sections[-1]\n",
    "\n",
    "    sectPr = section._sectPr\n",
    "    cols = sectPr.xpath('./w:cols')[0]\n",
    "    cols.set(qn('w:num'),'2')\n",
    "    doc.add_paragraph(t)\n",
    "\n",
    "    doc.add_section(0)\n",
    "    section = doc.sections[-1]\n",
    "\n",
    "    sectPr = section._sectPr\n",
    "    cols = sectPr.xpath('./w:cols')[0]\n",
    "    cols.set(qn('w:num'),'1')\n",
    "    doc.add_page_break()\n",
    "    \n",
    "# clear margins \n",
    "\n",
    "sections = doc.sections\n",
    "for section in sections:\n",
    "    section.top_margin = Inches(0.5)\n",
    "    section.bottom_margin = Inches(0.5)\n",
    "    section.left_margin = Inches(.75)\n",
    "    section.right_margin = Inches(.75)\n",
    "\n",
    "doc.save('test.docx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
